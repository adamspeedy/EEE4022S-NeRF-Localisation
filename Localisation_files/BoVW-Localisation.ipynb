{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bag of Visual Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-contrib-python opencv-python\n",
    "%pip install matplotlib\n",
    "%pip install scipy\n",
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#loads images into a list\n",
    "#image_directory = 'images/'\n",
    "#image_directory = 'Results-Frames-Uncropped/'\n",
    "\n",
    "#image_list = [Image.open(os.path.join(image_directory, img)) for img in os.listdir(image_directory)]\n",
    "\n",
    "image_list=[]\n",
    "\n",
    "#make sure to change this depending on how many images there are\n",
    "for i in range(0,869,1):\n",
    "    #img_data = PIL.Image.open('images/'+str(i)+'.png' )\n",
    "    img_data = PIL.Image.open('Results-Frames-Cropped//'+str(i)+'.png' )\n",
    "    #img_data = PIL.Image.open('cropped-images/Crop-'+str(i)+'.png' )\n",
    "    temp=np.array(img_data)\n",
    "    image_list.append(temp)\n",
    "image_data=np.array(image_list)\n",
    "\n",
    "#print(image_list[1])\n",
    "#print(image_list[2])\n",
    "\n",
    "#stores images into numpy arrays\n",
    "##image_array_list = [np.array(img) for img in image_list]\n",
    "#Put them all into one array\n",
    "#image_array = np.stack(image_array_list)\n",
    "\n",
    "\n",
    "imgplot = plt.imshow(image_list[50])\n",
    "#imgplot = plt.imshow(image_list[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# convert images to grayscale\n",
    "bw_images = []\n",
    "for img in image_list:\n",
    "    # if RGB, transform into grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "    else:\n",
    "        # if grayscale, do not transform\n",
    "        bw_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(bw_images[50], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Visual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction using ORB\n",
    "extractor = cv2.ORB_create(nfeatures=2000)\n",
    "\n",
    "# initialize lists where we will store *all* keypoints and descriptors\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "\n",
    "for img in bw_images:\n",
    "    # extract keypoints and descriptors for each image\n",
    "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
    "    keypoints.append(img_keypoints)\n",
    "    descriptors.append(img_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction using FAST\n",
    "# I get detect the feautures but still need to figure out how to\n",
    "# do the rest of the analysis given that I don't have descriptors\n",
    "extractor = cv2.FastFeatureDetector_create()\n",
    "extractor.setNonmaxSuppression(False)\n",
    "\n",
    "# initialize lists where we will store *all* keypoints and descriptors\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "\n",
    "for img in bw_images:\n",
    "    # extract keypoints and descriptors for each image\n",
    "    img_keypoints = extractor.detect(img, None)\n",
    "    keypoints.append(img_keypoints)\n",
    "    #descriptors.append(img_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction using SIFT\n",
    "#SURF is patented, so neeed to play around with the version of opencv or set \n",
    "#opencv-conrib support enabled to use it\n",
    "\n",
    "#extractor =cv2.xfeatures2d.SURF_create(400)\n",
    "extractor = cv2.xfeatures2d.SURF_create()    \n",
    "# Only features, whose hessian is larger than hessianThreshold are retained by the detector\n",
    "\n",
    "# initialize lists where we will store *all* keypoints and descriptors\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "\n",
    "for img in bw_images:\n",
    "    # extract keypoints and descriptors for each image\n",
    "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
    "    keypoints.append(img_keypoints)\n",
    "    descriptors.append(img_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction using SIFT\n",
    "extractor = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# initialize lists where we will store *all* keypoints and descriptors\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "\n",
    "for img in bw_images:\n",
    "    # extract keypoints and descriptors for each image\n",
    "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
    "    keypoints.append(img_keypoints)\n",
    "    descriptors.append(img_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start preparing the data for the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code just removes images that did not have any features identified\n",
    "#all images will most likely have something \n",
    "print(f\"len before: {len(descriptors)}\")\n",
    "# initialize list to store idx values of records to drop\n",
    "to_drop = []\n",
    "for i, img_descriptors in enumerate(descriptors):\n",
    "    # if there are no descriptors, add record idx to drop list\n",
    "    if img_descriptors is None:\n",
    "        to_drop.append(i)\n",
    "\n",
    "print(f\"indexes: {to_drop}\")\n",
    "# delete from list in reverse order\n",
    "for i in sorted(to_drop, reverse=True):\n",
    "    del descriptors[i], keypoints[i]\n",
    "\n",
    "print(f\"len after: {len(descriptors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the features Extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = []\n",
    "tempNum =0\n",
    "for x in range(0,860,100):\n",
    "    print(x)\n",
    "    output_image.append(cv2.drawKeypoints(bw_images[x], keypoints[x], 0, (255, 0, 0), flags=0))\n",
    "                                # flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
    "    plt.imshow(output_image[tempNum], cmap='gray')\n",
    "    tempNum+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# select the same numbers in each run\n",
    "np.random.seed(0)\n",
    "# select 1000 random image index values\n",
    "sample_idx = np.random.randint(0, len(image_list)+1, 95).tolist()\n",
    "len(sample_idx)\n",
    "\n",
    "\n",
    "# extract the sample from descriptors\n",
    "# (we don't need keypoints)\n",
    "descriptors_sample = []\n",
    "\n",
    "for m in sample_idx:\n",
    "    descriptors_sample.append(np.array(descriptors[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptors = []\n",
    "# extract image descriptor lists\n",
    "for img_descriptors in descriptors_sample:\n",
    "    # extract specific descriptors within the image\n",
    "    for descriptor in img_descriptors:\n",
    "        all_descriptors.append(descriptor)\n",
    "# convert to single numpy array\n",
    "all_descriptors = np.stack(all_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "all_descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can count the number of descriptors contained in descriptors to confirm\n",
    "count = []\n",
    "for img_descriptors in descriptors_sample:\n",
    "    count.append(len(img_descriptors))\n",
    "# here we can see the number of descriptors for the first five images\n",
    "print(f\"first five: {count[:5]}\")\n",
    "# and if we sum them all, we should see the 39893 from before\n",
    "print(f\"count all: {sum(count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-means clustering to build the codebook, takes some time tbh\n",
    "\n",
    "from scipy.cluster.vq import kmeans\n",
    "\n",
    "k = 500\n",
    "iters = 1\n",
    "#make sure to use .astype for ORB as it stores the description weird\n",
    "codebook, variance = kmeans((all_descriptors).astype(float), k, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# save number of clusters and codebook\n",
    "# Joblib dumps Python object into one file\n",
    "joblib.dump((k, codebook), \"ORB-Cropped-codebook-500.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the vocab back into the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the visual features, number of clusters, and codebook\n",
    "k, codebook = joblib.load(\"ORB-Cropped-codebook-500.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to add all of our training images in to do the localisatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is where you specify which images to test against\n",
    "average=0\n",
    "averageMin=0\n",
    "tenFound=0\n",
    "fiveFound=0\n",
    "oneFound=0\n",
    "count=1\n",
    "for indexImage in range(1,869,10):\n",
    "    print(\"testing for image: \"+str(indexImage))\n",
    "    train_image_list=[]\n",
    "    #make sure to change this depending on how many images there are\n",
    "    #for i in range(0,indexImage,indexImage+1):\n",
    "        #img_data = PIL.Image.open('images/'+str(i)+'.png' )\n",
    "    img_data = PIL.Image.open('cropped-images/Crop-'+str(indexImage)+'.png' )\n",
    "    #img_data = PIL.Image.open('images/'+str(indexImage)+'.png' )\n",
    "    temp=np.array(img_data)\n",
    "    train_image_list.append(temp)\n",
    "    image_data=np.array(train_image_list)\n",
    "\n",
    "    #plt.imshow(train_image_list[0])\n",
    "    #plt.show()\n",
    "    # convert images to grayscale\n",
    "\n",
    "    train_bw_images = []\n",
    "    for img in train_image_list:\n",
    "        # if RGB, transform into grayscale\n",
    "        if len(img.shape) == 3:\n",
    "            train_bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "        else:\n",
    "            # if grayscale, do not transform\n",
    "            train_bw_images.append(img)\n",
    "    #plt.imshow(train_bw_images[0], cmap='gray')\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "###### Feature Extraction using SIFT\n",
    "#    extractor = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # initialize lists where we will store *all* keypoints and descriptors\n",
    "#    train_keypoints = []\n",
    "#    train_descriptors = []\n",
    "\n",
    "#    for img in train_bw_images:\n",
    "#       # extract keypoints and descriptors for each image\n",
    "#        img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
    "#        train_keypoints.append(img_keypoints)\n",
    "#        train_descriptors.append(img_descriptors)\n",
    "\n",
    "\n",
    "##### feature Extraction using ORB\n",
    "    extractor = cv2.ORB_create(nfeatures=2000)\n",
    "#\n",
    "    # initialize lists where we will store *all* keypoints and descriptors\n",
    "    train_keypoints = []\n",
    "    train_descriptors = []\n",
    "\n",
    "    for img in train_bw_images:\n",
    "        # extract keypoints and descriptors for each image\n",
    "        img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
    "        train_keypoints.append(img_keypoints)\n",
    "        train_descriptors.append(img_descriptors)\n",
    "\n",
    "##### vector quantization\n",
    "    from scipy.cluster.vq import vq\n",
    "    import math\n",
    "\n",
    "    visual_words = []\n",
    "    for img_descriptors in descriptors:\n",
    "        # for each image, map each descriptor to the nearest codebook entry\n",
    "        img_visual_words, distance = vq(img_descriptors, codebook)\n",
    "        visual_words.append(img_visual_words)\n",
    "\n",
    "    #print(len(visual_words))\n",
    "    # Adding my Training image into the set of visual words\n",
    "    for img_descriptors in train_descriptors:\n",
    "        # for each image, map each descriptor to the nearest codebook entry\n",
    "        img_visual_words, distance = vq(img_descriptors, codebook)\n",
    "        visual_words.append(img_visual_words)\n",
    "\n",
    "    #print(len(visual_words))\n",
    "\n",
    "###### Frequency Count\n",
    "    k=500\n",
    "    frequency_vectors = []\n",
    "    for img_visual_words in visual_words:\n",
    "        # create a frequency vector for each image\n",
    "        img_frequency_vector = np.zeros(k)\n",
    "        for word in img_visual_words:\n",
    "            img_frequency_vector[word] += 1\n",
    "        frequency_vectors.append(img_frequency_vector)\n",
    "    # stack together in numpy array\n",
    "    frequency_vectors = np.stack(frequency_vectors)\n",
    "\n",
    "    frequency_vectors.shape\n",
    "\n",
    "##### Running tf-idf\n",
    "\n",
    "    N = 870 # N is the number of images, i.e. the size of the dataset\n",
    "\n",
    "    # df is the number of images that a visual word appears in\n",
    "    # we calculate it by counting non-zero values as 1 and summing\n",
    "    df = np.sum(frequency_vectors > 0, axis=0)\n",
    "    #idf = np.array()\n",
    "    #for i in df:\n",
    "    # if(df[i]>0):\n",
    "        #    idf[i] = np.log(N/ df[i])\n",
    "    # else:\n",
    "        #   idf[i]=0\n",
    "    idf = np.log(N/ df)\n",
    "    idf.shape, idf[:5]\n",
    "    #if you see negativs here chekc that you said the right size of the data set 2 cells above\n",
    "    tfidf = frequency_vectors * idf\n",
    "    tfidf.shape, tfidf[0][:5]\n",
    "\n",
    "    #important to replace all the nan values with 0\n",
    "    tfidf[np.isnan(tfidf)] = 0\n",
    "\n",
    "    # Now we can check to see the similarity of images\n",
    "    # cosine similarity\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "\n",
    "##### Chi Squared \n",
    "    search_i=869\n",
    "    #a = frequency_vectors[search_i]\n",
    "   # b = frequency_vectors#tfidf  # set search space to the full sample\n",
    "\n",
    "    a = tfidf[search_i]\n",
    "    b = tfidf  # set search space to the full sample\n",
    "    chi_similarity=[]\n",
    "    for i in range(0,870):\n",
    "        tempChi=0\n",
    "        for d in range(0,500):\n",
    "            #preventing a divide by 0 error\n",
    "            if(a[d]!=0):\n",
    "                tempChi+=((a[d]-b[i][d])**2)/a[d]\n",
    "        chi_similarity.append(tempChi)\n",
    "    chi_similarity=np.array(chi_similarity)\n",
    "   # print(\"Min Chi similarity:\", round(np.min(chi_similarity),1))\n",
    "   # print(\"Max Chi similarity:\", np.max(chi_similarity))\n",
    "   # print(\"Probability\"+str(chi_similarity[indexImage]))\n",
    "    average=average+chi_similarity[indexImage]\n",
    "   # print(\"running average: \"+str(average/count))\n",
    "\n",
    "##### Cosine Similarity\n",
    "  #  search_i=869\n",
    "    #a = frequency_vectors[search_i]#tfidf[search_i]\n",
    "    #b = frequency_vectors#tfidf  # set search space to the full sample\n",
    "\n",
    "   # a = tfidf[search_i]\n",
    "   # b = tfidf  # set search space to the full sample\n",
    "\n",
    "   # cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
    "#    print(\"Min cosine similarity:\", round(np.min(cosine_similarity),1))\n",
    "#    print(\"Max cosine similarity:\", np.max(cosine_similarity))\n",
    "#    cosine_similarity\n",
    "#    print(\"Probability\"+str(cosine_similarity[indexImage]))\n",
    "   # average=average+cosine_similarity[indexImage]\n",
    "#    print(\"running average: \"+str(average/count))\n",
    "\n",
    "\n",
    "#### Normalised Euclidean Distance\n",
    "    #testing Normalised Euclidean Distance\n",
    "    #remember the higher the distancee the less similar the images are:\n",
    "    #a = frequency_vectors\n",
    "    #b = frequency_vectors\n",
    "   # search_i=869\n",
    "   # a = tfidf[search_i]\n",
    "   # b = tfidf  # set search space to the full sample\n",
    "   # euclid_similarity = []\n",
    "   # for i in range(0,870):\n",
    "   #     tempEuc=0\n",
    "   #     for k in range(0,500):\n",
    "   #         tempEuc+=((a[k]-b[i][k])**2)/500\n",
    "   #     euclid_similarity.append(tempEuc)\n",
    "   # euclid_similarity=np.array(euclid_similarity)\n",
    "    #print(\"Min euclidean similarity:\", round(np.min(euclid_similarity),1))\n",
    "    #print(\"Max euclidean similarity:\", np.max(euclid_similarity))\n",
    "#    print(\"Probability\"+str(euclid_similarity[indexImage]))\n",
    "   # average=average+euclid_similarity[indexImage]\n",
    "#    print(\"running average: \"+str(average/count))\n",
    "    \n",
    "### Output Results\n",
    "\n",
    "    top_k = 5\n",
    "    #here we are sorting from highest to lowest\n",
    "    #use this if you are looking at the angle between histograms\n",
    "    #idx = np.argsort(-cosine_similarity)[:top_k]\n",
    "    idx = np.argsort(chi_similarity)[:top_k]\n",
    "    #idx = np.argsort(euclid_similarity)[:top_k]\n",
    "    print(idx)\n",
    "\n",
    "    #here we are sorting from lowest to highest\n",
    "    #use this if you are looking at distance between histograms\n",
    "    #idx = np.argsort(euclid_similarity)[:top_k]\n",
    "    #idx\n",
    "    #print(cosine_similarity[idx[1]])\n",
    "    #print(euclid_similarity[idx[1]])\n",
    "    print(chi_similarity[idx[1]])\n",
    "\n",
    "    for g in idx:\n",
    "        min=869\n",
    "        if(abs(idx[1]-indexImage)<min):\n",
    "            min=abs(idx[1]-indexImage)\n",
    "    if min<=10:\n",
    "        tenFound=tenFound+1\n",
    "    if min<=5:\n",
    "        fiveFound=fiveFound+1\n",
    "    if min<=2:\n",
    "        oneFound=oneFound+1\n",
    "    averageMin=averageMin+min\n",
    "    #print(\"current Min Value: \"+str(min))\n",
    "    #print(\"average Min Value: \"+str(averageMin/count))\n",
    "    count =count+1\n",
    "print(\"end average: \"+str(average/(count-1)))\n",
    "print(\"within One: \"+str(oneFound))\n",
    "print(\"within five: \"+str(fiveFound))\n",
    "print(\"within ten: \"+str(tenFound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is a culmination of the code shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector quantization\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "visual_words = []\n",
    "for img_descriptors in descriptors:\n",
    "    # for each image, map each descriptor to the nearest codebook entry\n",
    "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
    "    visual_words.append(img_visual_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the visual words look like for image 0\n",
    "visual_words[0][:5], len(visual_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the centroid that represents visual word 84 is of dimensionality...\n",
    "codebook[84].shape  # (all have the same dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vectors = []\n",
    "for img_visual_words in visual_words:\n",
    "    # create a frequency vector for each image\n",
    "    img_frequency_vector = np.zeros(k)\n",
    "    for word in img_visual_words:\n",
    "        if word<199:\n",
    "            img_frequency_vector[word] += 1\n",
    "    frequency_vectors.append(img_frequency_vector)\n",
    "# stack together in numpy array\n",
    "frequency_vectors = np.stack(frequency_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know from above that ids 84, 22, 45, and 172 appear in image 0\n",
    "for i in [84,  22,  45, 172]:\n",
    "    print(f\"{i}: {frequency_vectors[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(range(k)), frequency_vectors[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is the number of images, i.e. the size of the dataset\n",
    "N = 869\n",
    "\n",
    "# df is the number of images that a visual word appears in\n",
    "# we calculate it by counting non-zero values as 1 and summing\n",
    "df = np.sum(frequency_vectors > 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(N/ df)\n",
    "idf.shape, idf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = frequency_vectors * idf\n",
    "tfidf.shape, tfidf[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(range(k)), tfidf[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_i = 49\n",
    "\n",
    "plt.imshow(bw_images[search_i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for reference, our cosine similarity range is $[0,1]$, and it is calculated as follows:\n",
    "\n",
    "$$cossim(A,B)= cos(\\theta)=\\frac{A \\cdot B}{||A|| \\space ||B||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "a = tfidf[search_i]\n",
    "b = tfidf  # set search space to the full sample\n",
    "\n",
    "cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
    "print(\"Min cosine similarity:\", round(np.min(cosine_similarity),1))\n",
    "print(\"Max cosine similarity:\", np.max(cosine_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is testing Chi squared algorithm, still playing around though\n",
    " $$x^2=\\sum_{i=1}^n(\\frac{(a-b)^2}{a})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing Chi Square Distance\n",
    "# again the lower the distance the worse the comparison\n",
    "a = tfidf[869]\n",
    "b = tfidf  # set search space to the full sample\n",
    "chi_similarity=[]\n",
    "for i in range(0,869,1):\n",
    "    tempChi=0\n",
    "    for k in range(0,200):\n",
    "        #preventing a divide by 0 error\n",
    "        if(a[k]!=0):\n",
    "            tempChi+=((a[k]-b[i][k])**2)/a[k]\n",
    "    chi_similarity.append(tempChi)\n",
    "chi_similarity=np.array(chi_similarity)\n",
    "print(\"Min Chi similarity:\", round(np.min(chi_similarity),1))\n",
    "print(\"Max Chi similarity:\", np.max(chi_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using normalised euclidean distance: \n",
    "$$D=\\sqrt{\\sum_{i=1}^n\\frac{(a-b)^2}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing Normalised Euclidean Distance\n",
    "#remember the higher the distancee the less similar the images are:\n",
    "a = tfidf\n",
    "b = tfidf  # set search space to the full sample\n",
    "euclid_similarity = []\n",
    "for i in range(0,95):\n",
    "    tempEuc=0\n",
    "    for k in range(0,200):\n",
    "        tempEuc+=((a[0][k]-b[i][k])**2)/200\n",
    "    euclid_similarity.append(tempEuc)\n",
    "euclid_similarity=np.array(euclid_similarity)\n",
    "print(\"Min euclidean similarity:\", round(np.min(euclid_similarity),1))\n",
    "print(\"Max euclidean similarity:\", np.max(euclid_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cosine_similarity.shape)\n",
    "#print(euclid_similarity.shape)\n",
    "print(chi_similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine_similarity\n",
    "#euclid_similarity\n",
    "chi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "#here we are sorting from highest to lowest\n",
    "#use this if you are looking at the angle between histograms\n",
    "#idx = np.argsort(-euclid_similarity)[:top_k]\n",
    "\n",
    "#here we are sorting from lowest to highest\n",
    "#use this if you are looking at distance between histograms\n",
    "idx = np.argsort(euclid_similarity)[:top_k]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclid_similarity[idx[0]]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
